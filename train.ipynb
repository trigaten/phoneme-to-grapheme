{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoneme to Grapheme Conversion with a Recurrent Generative Model \n",
    "We hope the reader will appreciate the attempts at humor built in for subjective reasons.\n",
    "This project will discuss..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import pandas as pd\n",
    "# for logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# compute phoneme/grapheme vocabularies\n",
    "data = pd.read_csv(\"phonemes-words.csv\")\n",
    "phonemes_col = data[\"phonemes\"]\n",
    "graphemes_col = data[\"graphemes\"]\n",
    "# vocabularies contain 0 and 1 as start and end tokens\n",
    "phonemes = ['0', '1']\n",
    "graphemes = ['0', '1']\n",
    "\n",
    "for word in phonemes_col:\n",
    "    for phoneme in word:\n",
    "        if phoneme not in phonemes:\n",
    "            phonemes.append(phoneme)\n",
    "for word in graphemes_col:\n",
    "    for grapheme in word:\n",
    "        if grapheme not in graphemes:\n",
    "            graphemes.append(grapheme)\n",
    "            \n",
    "# wow, there are lot of different phonemes!\n",
    "print(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nemes_to_1_hot_seq(string, nemes=\"phonemes\"):\n",
    "    \"\"\"one hot encodes the word according to either\n",
    "    the phoneme or grapheme list\n",
    "        ::returns:: pytorch tensor of one hot encoded characters\n",
    "    \"\"\"\n",
    "    string = '0' + string + '1'\n",
    "    l = phonemes if nemes == \"phonemes\" else graphemes\n",
    "    seq = []\n",
    "    for i in string:\n",
    "        vec = [0] * len(l)\n",
    "        vec[l.index(i)] = 1\n",
    "        seq.append(vec)\n",
    "\n",
    "    return torch.FloatTensor([seq])\n",
    "\n",
    "def one_hot_to_nemes(arr, nemes=\"phonemes\"):\n",
    "    \"\"\"converts a 1-hot encoding back to characters\"\"\"\n",
    "    seq = []\n",
    "    l = phonemes if nemes == \"phonemes\" else graphemes\n",
    "    for hot in arr:\n",
    "        x = torch.argmax(hot)\n",
    "        seq.append(l[x])\n",
    "    return seq\n",
    "\n",
    "\n",
    "class P2GDataset(Dataset):\n",
    "    \"\"\"Pytorch dataset object for sampling the dataset\"\"\"\n",
    "    def __init__(self, phoneme_file, device):\n",
    "        df = pd.read_csv(phoneme_file)\n",
    "        self.data = df\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p, g = self.data.iloc[idx]\n",
    "        # 1-hot encoding\n",
    "        return nemes_to_1_hot_seq(p, nemes = \"phonemes\").to(self.device), nemes_to_1_hot_seq(g, nemes = \"graphemes\").long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "The model changed signifigantly over time. We tried the following combinations:\n",
    "\n",
    "Single layer GRU encoder+decoder\n",
    "\n",
    "Double/Triple stacked GRU encoder+decoder\n",
    "\n",
    "Single layer LSTM encoder+decoder\n",
    "\n",
    "We also varied hidden sizes, testing 512 or 1024\n",
    "\n",
    "We settled on the following architecture (hidden size 512):\n",
    "\n",
    "Double bidirectional stacked GRU encoder -> linear layer which accepts the last forward/backward hidden layers and converts them to a vector the size of a single hidden layer -> unstacked unidirectional GRU decoder with Bahdanau attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer size\n",
    "layer_size = 512\n",
    "# define model architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.GRU(len(phonemes), layer_size, 2, batch_first=True, bidirectional=True, dropout=0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            # takes final forwards and backwards hidden states\n",
    "            nn.Linear(2 * layer_size, layer_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # push vector through encoder\n",
    "        out, hidden = self.encoder(x)\n",
    "        # hidden is [4, 1, layer_size]\n",
    "        # this is because of bidirectionality * double stacked\n",
    "        # we want to grab the \"highest\" layers from the forwards and backwards directions\n",
    "        # dim 1 because hidden[3] and hidden[4] are both [1, layer_size] and we \n",
    "        # want a single batch that has 2*layer_size values\n",
    "        hc = torch.cat((hidden[2], hidden[3]), dim=1)\n",
    "        hidden_for_init = self.fc(hc)\n",
    "\n",
    "        # return context vector\n",
    "        return out, hidden_for_init\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 2 from encoder state and 1 from decoder state (since not bidirectional)\n",
    "        self.energy = nn.Sequential(\n",
    "            nn.Linear(3*layer_size, layer_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # map energy vectors to single values\n",
    "        self.attention = nn.Linear(layer_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, encoder_hiddens, decoder_hidden):\n",
    "        # encoder_hiddens is [1, L, layer_size*2] bc bidirectional\n",
    "        # decoder_hidden is [1, layer_size]\n",
    "        # 1 bc using batch first \n",
    "        num_encoder_hiddens = encoder_hiddens.shape[1]\n",
    "\n",
    "        # make it [1,1,layer_size]\n",
    "        decoder_hidden = torch.unsqueeze(decoder_hidden, 0)\n",
    "\n",
    "        # repeat along second dim to get [4, 1, layer_size]\n",
    "       \n",
    "        decoder_hiddens = decoder_hidden.squeeze(0).repeat(1, num_encoder_hiddens, 1)\n",
    "        \n",
    "        inputs = torch.cat((encoder_hiddens, decoder_hiddens), 2)\n",
    "\n",
    "        energy = self.energy(inputs)\n",
    "\n",
    "        attention = self.attention(energy)\n",
    "\n",
    "        # want a distribution of attention that sums to 1\n",
    "        return F.softmax(attention, dim=2)\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, attention):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        # decoder GRU takes in previous output word, attention vector, current hidden state\n",
    "        self.decoder = nn.GRU(len(graphemes) + 2*layer_size, layer_size, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(layer_size*3 + len(graphemes), len(graphemes))\n",
    "        )\n",
    "        \n",
    "    def forward(self, input, hidden_layer, encoder_hiddens):\n",
    "        \"\"\"\n",
    "        Since this function gets called once at a time rather than taking in\n",
    "        a sequence of vectors, we need to pass it the last output. This will be just\n",
    "        a vector of numbers that can be converted to the embedding representing that last output\n",
    "        \"\"\"\n",
    "        # [1,1,layer_size]\n",
    "        attention_vals = self.attention(encoder_hiddens, hidden_layer)\n",
    "        attention_vals = attention_vals.permute(0, 2, 1)\n",
    "\n",
    "        # encoder_hiddens [1,L,layer_size]\n",
    "        # this just multiplies each attention value against the appropriate vector\n",
    "        # and sums the weighted vectors\n",
    "        # will be [1, 1, layer_size]\n",
    "        attended = torch.bmm(attention_vals, encoder_hiddens)\n",
    "        input = torch.cat((attended, input), dim=2)\n",
    "        out, hidden = self.decoder(input, hidden_layer)\n",
    "        # out[1] to get top hidden layer\n",
    "        input_for_fc = torch.cat((input, out), dim = 2)\n",
    "\n",
    "        return self.fc(input_for_fc), hidden\n",
    "\n",
    "class seq2seq(nn.Module):\n",
    "    \"\"\"The seq2seq model itself\"\"\"\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        # instantiate encoder and decoder with attention\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(Attention())\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, in_seq, out_seq, tf_ratio=0.5):\n",
    "        \"\"\"\n",
    "        :param tf_ratio: is the teacher forcing ratio. It decides how frequently\n",
    "        the model receives its own previously predicted token as opposed to the\n",
    "        known correct token.\n",
    "        \"\"\"\n",
    "        out_len = out_seq.shape[1]\n",
    "        # storing the outputs of the sequence\n",
    "        outputs = torch.zeros(out_len, 1, len(graphemes)).to(self.device)\n",
    "\n",
    "        out_for_at, hidden = self.encoder(in_seq)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        out_seq = out_seq.squeeze(0)\n",
    "\n",
    "        # perform an embarassing amount of data conversions\n",
    "        input = out_seq[0].unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "        \n",
    "        # for each token in known out sequence (except the first)\n",
    "        for i in range(1, out_len):\n",
    "            out, hidden = self.decoder(input, hidden, out_for_at)\n",
    "            outputs[i] = out\n",
    "\n",
    "            if random.random() > tf_ratio:\n",
    "                # teacher forcing (make next input what the current output token should be)\n",
    "                input = out_seq[i].unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "            else:\n",
    "                # use previously output token\n",
    "                x = input.argmax(1)[0]\n",
    "                input = torch.zeros(1, 1, len(graphemes)).to(self.device)\n",
    "                input[0][0][x] = 1\n",
    "                \n",
    "        return outputs\n",
    "\n",
    "    def pred_new(self, in_seq):\n",
    "        \"\"\"Method to predict the output sequence for a previously unseen\n",
    "        input sequence. The main difference between this function and forward\n",
    "        is that this function only stops decoding when the model produces and\n",
    "        end token\n",
    "        \"\"\"\n",
    "        encoder_out_for_at, hidden = self.encoder(in_seq)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        input = torch.zeros(1, 1, len(graphemes)).to(self.device)\n",
    "        outs = []\n",
    "        while True:\n",
    "            out, hidden = self.decoder(input, hidden, encoder_out_for_at)\n",
    "            outs.append(out)\n",
    "            # in case not hitting end token\n",
    "            if len(outs) > 50:\n",
    "                break\n",
    "            x = input.argmax(1)[0]\n",
    "            input = torch.zeros(1, 1, len(graphemes)).to(self.device)\n",
    "            input[0][0][x] = 1\n",
    "            if one_hot_to_nemes(out) == ['1']:\n",
    "                break\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizer/loss func/hyperparams\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 15\n",
    "model = seq2seq(device).to(device)\n",
    "# what a beautiful architecture\n",
    "print(\"Model architecture \", model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "dataset = P2GDataset(\"phonemes-words.csv\", device)\n",
    "# train on 70000 words\n",
    "train, test = random_split(dataset, [70000, len(dataset)-70000])\n",
    "dataloader = DataLoader(dataset=train, batch_size=1)\n",
    "print(\"train size \", len(train))\n",
    "print(\"test size \", len(test))\n",
    "\n",
    "def get_0_1_accuracy(test_set, model):\n",
    "    \"\"\"method to compute 1-WER accuracy AKA what % of test_set does model get\n",
    "    exactly correct.\"\"\"\n",
    "    correct = 0\n",
    "    dataloader = DataLoader(dataset=test_set, batch_size=1)\n",
    "    for (in_seq, out_seq) in dataloader:\n",
    "        prediction = model.pred_new(in_seq[0])\n",
    "        true = \"\".join(one_hot_to_nemes(out_seq[0][0], \"graphemes\"))[1:-1]\n",
    "        print(true)\n",
    "        pred = \"\".join(one_hot_to_nemes(prediction, \"graphemes\"))[0:-1]\n",
    "        print(pred)\n",
    "        \n",
    "        if true == pred:\n",
    "            correct+= 1\n",
    "    if correct == 0:\n",
    "        return correct\n",
    "    return correct/len(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of model parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow 10 million params! This model has more trainable parameters than tonnes of potatoes France produced in 2016! (absolutely no semantic relation). This might take a while to train, so make sure to use a NVIDIA GeForce RTX 2080 Ti :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"tensorboard_data\")\n",
    "# get a mini testing batch to check model accuracy on the test set\n",
    "# throughout training\n",
    "# NOTE: this is not a validation set\n",
    "_, mini_test = random_split(test, [20, len(test)-20])\n",
    "\n",
    "# begin training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    tot_loss = 0\n",
    "    for (in_seq, out_seq) in dataloader:\n",
    "        # batch size of 1\n",
    "        in_seq = in_seq.squeeze(0)\n",
    "        out_seq = out_seq.squeeze(0)\n",
    "        # perform inference\n",
    "        model_output = model(in_seq, out_seq)\n",
    "        # dont compute loss using first token of in/out sequence\n",
    "        model_output = model_output[1:]\n",
    "        model_output = model_output.squeeze(1)\n",
    "        out_seq = out_seq.squeeze(0)[1:]\n",
    "        # compute loss\n",
    "        loss = loss_func(model_output, out_seq.argmax(1).to(device))\n",
    "        # record loss\n",
    "        tot_loss+=loss.detach().item()\n",
    "        # accumulate gradients\n",
    "        loss.backward()\n",
    "        # step and clear grads\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    tot_loss/=len(train)\n",
    "    # record current accuracy on test set and average loss\n",
    "    writer.add_scalar(\"tensorboard_data/acc\", get_0_1_accuracy(mini_test, model), epoch)\n",
    "    writer.add_scalar(\"tensorboard_data/loss\", tot_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn dropout off\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Test accuracy: \" + str(get_0_1_accuracy(test, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that accuracy is... okay. It might be better than the average human (when faced with 10s of thousands of words), but thats still a lot of error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](secret_ingredient.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.encoder.weight_ih_l0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "If the reader would like more resources related to this topic:\n",
    "\n",
    "For learning the basics of RNNs, LSTMs, GRUs, attention (including Bahdanau), and seq2seq architectures, these resources are good:\n",
    "\n",
    "https://www.deeplearningbook.org/contents/rnn.html\n",
    "\n",
    "https://d2l.ai/chapter_recurrent-modern/seq2seq.html\n",
    "\n",
    "https://d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html\n",
    "\n",
    "For more comprehensive tutorials that walk through the full deep learning process (including varied seq2seq architectures such as transformer), this is a good resource:\n",
    "\n",
    "https://github.com/bentrevett/pytorch-seq2seq\n",
    "\n",
    "These papers discuss grapheme->phoneme conversion with deep learning. This is an easier problem, but still requires complex models for high success rates:\n",
    "\n",
    "https://arxiv.org/abs/2004.06338\n",
    "\n",
    "https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43264.pdf"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99cbaf4c95006fdb5479aa5c5d34049fe62ad37116b43f87b95900136f6751f7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
