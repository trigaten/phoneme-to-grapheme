{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "# known phonemes/graphemes\n",
    "phonemes = [\n",
    "    '0',\n",
    "    'ō', \n",
    "    'ē',\n",
    "    'f',\n",
    "    '1'\n",
    "]\n",
    "\n",
    "graphemes = [\n",
    "    '0', 'a', 'b', '1'\n",
    "]\n",
    "\n",
    "# one hot encodes the word: returns an array of one hot encoded characters\n",
    "def nemes_to_1_hot_seq(string, nemes=\"phonemes\"):\n",
    "    string = '0' + string + '1'\n",
    "    l = phonemes if nemes == \"phonemes\" else graphemes\n",
    "    seq = []\n",
    "    for i in string:\n",
    "        vec = [0] * len(l)\n",
    "        vec[l.index(i)] = 1\n",
    "        seq.append(vec)\n",
    "\n",
    "    return torch.FloatTensor([seq])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.GRU(len(phonemes), 512, 1, batch_first=True, bidirectional=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # push vector through encoder\n",
    "        out, h_n = self.encoder(x)\n",
    "\n",
    "        # return context vector\n",
    "        return h_n\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.GRU(len(graphemes), 512, 1, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(512, len(graphemes))\n",
    "\n",
    "    def forward(self, input, hidden_layer):\n",
    "        \"\"\"\n",
    "        Since this function gets called once at a time rather than taking in\n",
    "        a sequence of vectors, we need to pass it the last output. This will be just\n",
    "        a vector of numbers that can be converted to the embedding representing that last output\n",
    "        \"\"\"\n",
    "        out, h_n = self.decoder(input, hidden_layer)\n",
    "        # print(\"H\")\n",
    "        return self.fc(h_n), h_n\n",
    "\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, in_seq, out_seq, tf_ratio=0.5):\n",
    "        out_len = out_seq.shape[1]\n",
    "        # storing the outputs of the sequence\n",
    "        outputs = torch.zeros(out_len, 1, len(graphemes)).to(self.device)\n",
    "\n",
    "        hidden = self.encoder(in_seq)\n",
    "\n",
    "        out_seq = out_seq.squeeze(0)\n",
    "\n",
    "        input = out_seq[0].unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        for i in range(1, out_len):\n",
    "            out, hidden = self.decoder(input, hidden)\n",
    "            outputs[i] = out\n",
    "\n",
    "            if random.random() > tf_ratio:\n",
    "                # teacher forcing (make next input what the current output token should be)\n",
    "                input = out_seq[i].unsqueeze(0).unsqueeze(0)\n",
    "            else:\n",
    "                x = input.argmax(1)[0]\n",
    "                input = torch.zeros(1, 1, len(graphemes))\n",
    "                input[0][0][x] = 1\n",
    "                \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq(\n",
      "  (encoder): Encoder(\n",
      "    (encoder): GRU(5, 512, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder): GRU(4, 512, batch_first=True)\n",
      "    (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"training\"\"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 100\n",
    "model = seq2seq(device)\n",
    "# what a beautiful architecture\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# dataset OBJ!\n",
    "\n",
    "def train(epochs, )\n",
    "# print(x(nemes_to_1_hot_seq(\"ff\"), nemes_to_1_hot_seq('a', \"graphemes\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "in_seq = nemes_to_1_hot_seq(\"ff\")\n",
    "out_seq = nemes_to_1_hot_seq('a', \"graphemes\")\n",
    "\n",
    "tf_ratio=0.5\n",
    "\n",
    "out_len = out_seq.shape[0]\n",
    "# for storing\n",
    "outputs = torch.zeros(out_len, 1, len(graphemes))\n",
    "\n",
    "hidden = encoder(in_seq)\n",
    "input = out_seq[0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phonemes_to_1_hot_seq(string):\n",
    "    seq = []\n",
    "    for i in string:\n",
    "        vec = [0] *  len(phonemes)\n",
    "        vec[phonemes.index(i)] = 1\n",
    "        seq.append(vec)\n",
    "\n",
    "    return torch.FloatTensor([seq])\n",
    "\n",
    "vec = phonemes_to_1_hot_seq(\"ōō\")\n",
    "\n",
    "e = Encoder ()\n",
    "d = Decoder ()\n",
    "\n",
    "hidden = e(vec)\n",
    "\n",
    "previous_output = torch.zeros(1, 1, len(graphemes))\n",
    "\n",
    "previous_output, hidden = d(previous_output, hidden)\n",
    "\n",
    "# previous_output = previous_output.argmax(1) \n",
    "\n",
    "# decoder = nn.GRU(len(graphemes), 512, 1, batch_first=True, bidirectional=False)\n",
    "\n",
    "# output, hidden = decoder(previous_output, hidden)\n",
    "\n",
    "# fc = nn.Linear(512, len(graphemes))\n",
    "\n",
    "# fc(hidden)\n",
    "# d(e(vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0457,  0.0219,  0.0179, -0.0148]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previous_output.shape\n",
    "previous_output, hidden = d(previous_output, hidden)\n",
    "previous_output\n",
    "# previous_output, hidden = d(previous_output, hidden)\n",
    "# previous_output.argmax(3) \n",
    "# previous_output = previous_output.argmax(1) \n",
    "# print(previous_output)\n",
    "# previous_output, hidden = decoder(previous_output, hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0233,  0.0011, -0.0088,  ...,  0.0153,  0.0336, -0.0280],\n",
       "          [ 0.0108,  0.0076, -0.0168,  ...,  0.0231,  0.0282, -0.0414]]],\n",
       "        grad_fn=<TransposeBackward1>),\n",
       " tensor([[[ 1.0777e-02,  7.6281e-03, -1.6848e-02,  4.1977e-03, -1.2871e-02,\n",
       "            3.1418e-03,  2.1550e-02, -5.7571e-03, -3.0954e-02,  1.0037e-02,\n",
       "            2.6939e-02,  9.0874e-03,  1.7136e-02,  2.6673e-02, -1.0385e-02,\n",
       "           -1.8989e-02,  3.2243e-02, -1.9373e-02,  8.1069e-03, -1.0501e-02,\n",
       "           -4.1139e-04,  1.3408e-02, -6.0382e-03,  3.1077e-02, -5.5523e-03,\n",
       "            2.1457e-02,  1.1449e-02,  4.0108e-02,  3.2752e-02,  1.7254e-02,\n",
       "            1.0245e-02,  4.0089e-03,  3.5461e-02,  2.5902e-03,  2.3008e-02,\n",
       "            7.3063e-03, -1.6150e-03, -1.1977e-02, -3.3592e-02, -1.1931e-02,\n",
       "           -2.9569e-02, -4.2114e-02,  1.3337e-02, -6.8814e-03, -3.8481e-02,\n",
       "            1.5909e-03, -3.7625e-03,  1.6526e-02, -1.2831e-02,  9.7519e-03,\n",
       "           -8.3313e-03, -1.1014e-02,  5.0301e-04, -1.1268e-02,  3.2516e-02,\n",
       "            1.3713e-02, -6.0456e-03,  1.1432e-02,  2.7894e-02,  2.2075e-02,\n",
       "            4.8476e-03,  2.9172e-02, -1.0679e-02, -1.2577e-02, -1.6568e-02,\n",
       "           -2.5442e-02, -2.2785e-02, -3.0879e-02, -3.0658e-02,  2.1400e-02,\n",
       "           -2.4379e-02,  1.2242e-02, -2.9042e-02, -3.5087e-02, -2.4080e-03,\n",
       "           -1.3825e-02,  2.7545e-02,  2.3159e-02, -8.1452e-03, -1.3928e-02,\n",
       "            3.8943e-02,  2.1559e-02, -1.5341e-02,  2.3593e-02, -3.1340e-02,\n",
       "            3.8105e-02,  2.5909e-02,  1.1146e-02, -6.0744e-03, -5.4063e-03,\n",
       "            5.6450e-03, -6.8470e-03,  2.6413e-02, -2.8683e-03,  1.2066e-02,\n",
       "           -3.0895e-02,  3.6677e-02, -3.8261e-03,  1.4588e-02, -1.5052e-02,\n",
       "           -2.7361e-03, -5.0089e-03,  3.2140e-02, -1.3264e-02, -1.8796e-02,\n",
       "           -2.7330e-02, -2.1050e-02, -2.6772e-03, -3.5737e-02,  5.0795e-05,\n",
       "            2.6149e-02,  1.4522e-02,  3.8899e-02,  1.3441e-02,  1.7171e-02,\n",
       "            2.2216e-02,  2.3748e-02, -6.0926e-03,  2.3316e-02,  3.4886e-02,\n",
       "           -3.7015e-02,  4.1742e-02,  4.8248e-03, -3.3664e-03, -1.1656e-02,\n",
       "           -2.8434e-02, -3.8852e-02, -1.3802e-02, -1.3837e-02, -5.4367e-02,\n",
       "           -2.3792e-02, -5.0056e-02,  8.2534e-03,  1.9505e-02, -5.5053e-02,\n",
       "            3.1013e-02,  8.2096e-04, -3.0429e-02, -1.0907e-02, -7.6105e-03,\n",
       "           -1.7429e-02, -4.6293e-02, -8.3364e-03, -2.6651e-02,  8.4324e-03,\n",
       "            4.7508e-02,  7.2266e-04, -1.6009e-02,  1.5300e-02, -1.8964e-03,\n",
       "            7.5159e-03,  1.1751e-03,  1.2019e-02,  2.0969e-03,  1.8200e-02,\n",
       "            3.7415e-03, -4.2174e-02,  3.1244e-02,  6.9189e-03, -7.3491e-03,\n",
       "            9.2879e-03,  2.2042e-02, -4.7767e-02, -3.3415e-02,  1.7733e-02,\n",
       "           -1.1388e-03, -1.0373e-02,  2.4273e-02,  3.4199e-02,  2.2687e-03,\n",
       "            4.4304e-02, -1.3118e-02,  2.2061e-02,  4.1111e-03,  1.9231e-02,\n",
       "           -2.2990e-02,  3.4205e-03, -4.3264e-02, -8.0842e-03,  2.2720e-02,\n",
       "            1.7065e-02,  2.1761e-02, -3.6752e-03, -3.1528e-02,  2.9716e-02,\n",
       "            2.0226e-02,  1.6891e-02, -1.6837e-02, -3.8312e-03,  9.4046e-03,\n",
       "           -2.7853e-02, -1.0129e-02, -2.8473e-02,  4.1471e-02,  3.0655e-02,\n",
       "           -1.1658e-02, -1.5080e-02, -1.1479e-02,  2.8987e-02, -7.1413e-04,\n",
       "           -2.3145e-02,  1.7446e-02,  3.8535e-02, -9.2855e-03,  3.4428e-03,\n",
       "           -2.1968e-03,  2.8152e-02,  2.6754e-02,  2.7004e-02,  4.2673e-02,\n",
       "            4.6217e-03,  3.7353e-04, -1.0158e-02,  2.4218e-02,  7.4706e-03,\n",
       "            3.1534e-02,  2.1100e-02, -1.6329e-02, -3.2552e-02,  1.4981e-02,\n",
       "            1.4611e-02,  2.3745e-02, -3.6050e-02, -3.2865e-02, -2.2027e-02,\n",
       "            1.1067e-02,  5.2343e-02, -2.9432e-03, -1.1877e-02,  4.4793e-02,\n",
       "            1.6796e-02,  3.8566e-04,  2.9938e-02,  6.7775e-03,  2.4959e-02,\n",
       "           -2.2331e-02, -2.1253e-02,  1.8515e-02, -7.7439e-03, -1.2830e-02,\n",
       "           -1.1433e-02,  2.8782e-02, -3.2473e-02, -3.7484e-02, -3.5107e-02,\n",
       "           -9.2315e-04,  1.2475e-02, -1.6289e-02, -1.0266e-02, -5.2242e-03,\n",
       "           -1.2019e-02,  2.9344e-02,  2.1626e-02, -7.6953e-03, -1.0223e-02,\n",
       "            4.4816e-02,  2.2595e-02,  3.5656e-02,  2.7315e-02,  1.7945e-02,\n",
       "           -2.2972e-02, -3.7588e-02, -2.1444e-02, -1.2690e-02,  1.8650e-02,\n",
       "            3.4522e-03,  2.1491e-02, -1.0756e-02, -1.9640e-02, -7.1058e-03,\n",
       "            2.6034e-02,  1.9440e-02,  4.3051e-02, -6.0394e-03, -4.1497e-03,\n",
       "            3.5351e-02, -2.8727e-02, -3.2162e-03,  4.5031e-02, -1.8146e-02,\n",
       "           -3.2472e-03, -1.2373e-02,  5.8534e-03, -1.9729e-02,  4.4692e-02,\n",
       "            1.9181e-03, -9.2451e-03, -1.8264e-02, -2.5359e-02, -1.7916e-02,\n",
       "            1.6728e-03, -1.7803e-02, -1.5074e-02,  2.7302e-02, -1.1157e-02,\n",
       "            2.4707e-02,  9.8339e-04,  3.6940e-03,  1.9071e-02,  2.9353e-02,\n",
       "           -3.7830e-02,  4.3086e-02,  1.3685e-02, -2.6387e-02,  2.0901e-02,\n",
       "           -3.3127e-02, -4.1902e-02,  4.7872e-03,  4.0230e-02, -9.6783e-03,\n",
       "            2.1937e-02, -3.0535e-02,  1.4630e-02, -1.4717e-02,  4.2450e-03,\n",
       "            1.5193e-02,  2.0042e-02,  3.1653e-02,  4.4109e-03,  9.9181e-03,\n",
       "            5.4529e-03,  2.5318e-02,  2.1617e-03, -1.8594e-02, -1.2230e-02,\n",
       "           -1.1778e-02,  9.2439e-03, -7.2606e-04,  1.2930e-02, -1.6859e-02,\n",
       "           -3.0238e-02,  2.3568e-02,  4.3930e-03,  6.3508e-03, -9.2678e-03,\n",
       "           -2.3188e-02, -2.3598e-02,  5.3504e-03, -1.9066e-02, -1.9036e-02,\n",
       "            9.6259e-04, -1.0931e-03, -3.7319e-02, -2.5592e-02, -6.7342e-03,\n",
       "            2.1283e-02, -1.5608e-02,  1.6123e-02, -1.1714e-02,  3.4286e-02,\n",
       "            1.8451e-02, -1.0726e-02, -1.3681e-02, -3.8264e-03, -9.9757e-03,\n",
       "            3.1948e-03, -2.3392e-02, -1.8613e-02,  1.7151e-02, -2.0972e-02,\n",
       "            2.6653e-02,  7.7686e-04,  3.3073e-02, -3.5182e-02,  3.0931e-03,\n",
       "           -2.9191e-02,  7.6150e-03, -2.0654e-02, -3.4187e-02, -6.6356e-03,\n",
       "           -5.6253e-03, -5.6271e-02, -1.0318e-02, -3.7333e-03, -1.2689e-02,\n",
       "            3.2795e-02,  1.3084e-02,  1.0584e-02, -1.3468e-02,  2.1495e-02,\n",
       "            2.3166e-02, -1.3983e-02,  7.9417e-03,  1.5073e-02, -2.7238e-02,\n",
       "           -1.1314e-02, -3.7802e-02,  4.0761e-02,  3.2237e-02, -5.3593e-02,\n",
       "            8.4517e-03,  3.9334e-02, -1.0609e-02,  3.3642e-02, -1.5593e-02,\n",
       "            2.8125e-02, -2.9870e-02, -8.5236e-03,  3.8873e-02, -3.1461e-02,\n",
       "            1.9001e-02, -5.8408e-03,  6.6434e-03,  3.1510e-02, -1.9151e-02,\n",
       "           -1.7065e-02, -1.6026e-02, -9.9934e-03,  1.8644e-03, -1.8372e-03,\n",
       "            2.5351e-02, -2.5450e-02, -1.9076e-02,  1.3004e-02,  1.2562e-03,\n",
       "            2.2484e-02, -2.4972e-02, -2.2842e-02, -3.3060e-02, -9.7348e-03,\n",
       "            1.6707e-02,  1.7325e-03,  5.1017e-03, -3.3015e-02, -1.3905e-02,\n",
       "           -4.3555e-02, -3.5299e-03, -2.1306e-02,  3.3091e-02, -5.7693e-03,\n",
       "           -8.4036e-03,  2.8175e-02,  2.9301e-03,  3.5204e-03, -4.6313e-02,\n",
       "            4.7325e-02, -9.4605e-03, -3.0355e-03,  2.7185e-02, -3.8803e-02,\n",
       "            4.2043e-03, -3.3681e-02,  3.2197e-02,  2.8393e-02, -3.2638e-02,\n",
       "           -2.9434e-02, -2.4882e-02, -2.8745e-03, -2.3166e-03, -4.8971e-02,\n",
       "            2.3737e-02, -1.2619e-02, -1.6613e-02,  3.1855e-03,  3.3562e-02,\n",
       "            6.0534e-03,  1.6621e-02, -2.0841e-02,  3.5697e-02, -2.8896e-03,\n",
       "           -2.5939e-02,  1.5239e-02, -9.4360e-04, -1.0518e-02, -1.0681e-02,\n",
       "           -1.6903e-03, -2.5907e-02,  1.1489e-02,  3.6852e-02,  5.8475e-03,\n",
       "           -6.6079e-03, -1.0171e-02,  2.5252e-02, -1.9789e-02,  8.1776e-03,\n",
       "            1.9960e-02,  3.9872e-02, -1.4845e-02,  4.1220e-02, -2.9977e-03,\n",
       "            2.0940e-03, -7.2359e-03,  1.0195e-02, -2.0071e-02, -7.9671e-03,\n",
       "            1.7645e-02,  2.5512e-02,  4.4812e-05, -2.0835e-02,  5.9071e-03,\n",
       "           -4.2082e-02,  5.6698e-03,  2.5204e-02,  3.0913e-03,  1.8566e-02,\n",
       "            1.2476e-02,  4.9708e-02,  2.2925e-02,  2.0298e-02,  2.3789e-02,\n",
       "           -5.8592e-03,  2.8703e-02, -8.8888e-03, -6.5404e-03,  2.1433e-02,\n",
       "            3.0898e-02,  2.9007e-02,  3.7729e-03,  2.3192e-02,  2.3054e-02,\n",
       "            2.8183e-02, -4.1368e-02]]], grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = e(vec)\n",
    "\n",
    "previous_output = torch.zeros(1, 2, len(graphemes))\n",
    "# previous_output.shape\n",
    "# hidden.shape\n",
    "# d(previous_output, hidden)\n",
    "\n",
    "decoder = nn.GRU(len(graphemes), 512, 1, batch_first=True, bidirectional=False)\n",
    "\n",
    "decoder(previous_output, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'ē'\n",
    "seq = []\n",
    "for i in string:\n",
    "    vec = [0] *  len(phonemes)\n",
    "    vec[phonemes.index(i)] = 1\n",
    "    seq.append(vec)\n",
    "[seq]\n",
    "torch.FloatTensor([seq])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ae319d47093406129daff46394699ca00a0ff25db27d36a04b5bb5e9dd8e2e1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('env-01': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
