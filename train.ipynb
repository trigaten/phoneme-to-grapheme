{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoneme to Grapheme Conversion with a Recurrent Generative Model \n",
    "This project will discuss..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "# known phonemes/graphemes\n",
    "phonemes = [\n",
    "    '0',\n",
    "    'ō', \n",
    "    'ē',\n",
    "    'f',\n",
    "    '1'\n",
    "]\n",
    "\n",
    "graphemes = [\n",
    "    '0', 'a', 'b', '1'\n",
    "]\n",
    "\n",
    "# one hot encodes the word: returns an array of one hot encoded characters\n",
    "def nemes_to_1_hot_seq(string, nemes=\"phonemes\"):\n",
    "    string = '0' + string + '1'\n",
    "    l = phonemes if nemes == \"phonemes\" else graphemes\n",
    "    seq = []\n",
    "    for i in string:\n",
    "        vec = [0] * len(l)\n",
    "        vec[l.index(i)] = 1\n",
    "        seq.append(vec)\n",
    "\n",
    "    return torch.FloatTensor([seq])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.GRU(len(phonemes), 512, 1, batch_first=True, bidirectional=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # push vector through encoder\n",
    "        out, h_n = self.encoder(x)\n",
    "\n",
    "        # return context vector\n",
    "        return h_n\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.GRU(len(graphemes), 512, 1, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(512, len(graphemes))\n",
    "\n",
    "    def forward(self, input, hidden_layer):\n",
    "        \"\"\"\n",
    "        Since this function gets called once at a time rather than taking in\n",
    "        a sequence of vectors, we need to pass it the last output. This will be just\n",
    "        a vector of numbers that can be converted to the embedding representing that last output\n",
    "        \"\"\"\n",
    "        out, h_n = self.decoder(input, hidden_layer)\n",
    "        # print(\"H\")\n",
    "        return self.fc(h_n), h_n\n",
    "\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, in_seq, out_seq, tf_ratio=0.5):\n",
    "        out_len = out_seq.shape[1]\n",
    "        # storing the outputs of the sequence\n",
    "        outputs = torch.zeros(out_len, 1, len(graphemes)).to(self.device)\n",
    "\n",
    "        hidden = self.encoder(in_seq)\n",
    "\n",
    "        out_seq = out_seq.squeeze(0)\n",
    "\n",
    "        input = out_seq[0].unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        for i in range(1, out_len):\n",
    "            out, hidden = self.decoder(input, hidden)\n",
    "            outputs[i] = out\n",
    "\n",
    "            if random.random() > tf_ratio:\n",
    "                # teacher forcing (make next input what the current output token should be)\n",
    "                input = out_seq[i].unsqueeze(0).unsqueeze(0)\n",
    "            else:\n",
    "                x = input.argmax(1)[0]\n",
    "                input = torch.zeros(1, 1, len(graphemes))\n",
    "                input[0][0][x] = 1\n",
    "                \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq(\n",
      "  (encoder): Encoder(\n",
      "    (encoder): GRU(5, 512, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder): GRU(4, 512, batch_first=True)\n",
      "    (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.0344, -0.0213, -0.0112,  0.0098],\n",
      "        [-0.0370, -0.0186, -0.0226, -0.0061]], grad_fn=<SqueezeBackward1>)\n",
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.3824, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"training\"\"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 100\n",
    "model = seq2seq(device)\n",
    "# what a beautiful architecture\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "in_seq = nemes_to_1_hot_seq(\"ōō\")\n",
    "out_seq = nemes_to_1_hot_seq(\"a\", \"graphemes\")\n",
    "\n",
    "\n",
    "model_output = model(in_seq, out_seq)\n",
    "# get rid of first items because they are the start token,\n",
    "# which shouldnt be included in loss\n",
    "model_output = model_output[1:]\n",
    "model_output = model_output.squeeze(1)\n",
    "out_seq = out_seq[1:]\n",
    "print(model_output)\n",
    "print(model_output.shape)\n",
    "loss = loss_func(model_output, torch.LongTensor([1, 3]))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "loss.clear()\n",
    "\n",
    "# print(model(in_seq, out_seq))\n",
    "# print(out_seq)\n",
    "\n",
    "# loss_func(model(in_seq, out_seq).squeeze(0), torch.FloatTensor(0, 1, 1))\n",
    "# dataset OBJ!\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     for (in_seq, out_seq) in dataloader():\n",
    "#         outputs = model(in_seq, out_seq)\n",
    "# print(x(nemes_to_1_hot_seq(\"ff\"), nemes_to_1_hot_seq('a', \"graphemes\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_graphemes(phonemes):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ae319d47093406129daff46394699ca00a0ff25db27d36a04b5bb5e9dd8e2e1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('env-01': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
