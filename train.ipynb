{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoneme to Grapheme Conversion with a Recurrent Generative Model \n",
    "We hope the reader will appreciate the attempts at humor built in for subjective reasons.\n",
    "This project will discuss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "The model change signifigantly over time. We tried the following combinations:\n",
    "\n",
    "Single layer GRU encoder+decoder\n",
    "\n",
    "Double/Triple stacked GRU encoder+decoder\n",
    "\n",
    "Single layer LSTM encoder+decoder\n",
    "\n",
    "We also varied hidden sizes, testing 512 or 1024\n",
    "\n",
    "We settled on the following architecture (hidden size 512):\n",
    "\n",
    "Double bidirectional stacked GRU encoder -> linear layer which accepts the last forward/backward hidden layers and converts them to a vector the size of a single hidden layer -> unstacked unidirectional GRU decoder with Bahdanau attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "For learning the basics of RNNs, LSTMs, GRUs, attention (including Bahdanau), and seq2seq architectures, these resources are good:\n",
    "https://www.deeplearningbook.org/contents/rnn.html\n",
    "https://d2l.ai/chapter_recurrent-modern/seq2seq.html\n",
    "https://d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html\n",
    "For more comprehensive tutorials that walk through the full deep learning process (including varied seq2seq architectures such as transformer), this is a good resource:\n",
    "https://github.com/bentrevett/pytorch-seq2seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', 'k', 'y', 'ʊ', 'r', 'ɑ', 'a', 'ɪ', 'z', 'æ', 't', 'ə', 'b', 'n', 'u', 'l', 'm', 'ɛ', 'd', 'e', 'ʃ', 's', 'ʌ', 'g', 'f', 'o', 'θ', 'ɒ', 'i', 'ʒ', 'p', 'ɔ', 'v', 'ː', 'h', 'ŋ', 'w', '̃', 'ɡ', 'j', 'ɜ', 'ð', 'ʰ', 'x', 'c', 'œ', 'ü', 'ɘ', 'ø', 'ĩ']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# find phoneme vocabulary\n",
    "data = pd.read_csv(\"phonemes-words.csv\")\n",
    "phonemes_col = data[\"phonemes\"]\n",
    "graphemes_col = data[\"graphemes\"]\n",
    "phonemes = ['0', '1']\n",
    "graphemes = ['0', '1']\n",
    "\n",
    "for word in phonemes_col:\n",
    "    # print(word)\n",
    "    for phoneme in word:\n",
    "        if phoneme not in phonemes:\n",
    "            phonemes.append(phoneme)\n",
    "for word in graphemes_col:\n",
    "    # print(word)\n",
    "    for grapheme in word:\n",
    "        if grapheme not in graphemes:\n",
    "            graphemes.append(grapheme)\n",
    "print(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# one hot encodes the word: returns an array of one hot encoded characters\n",
    "def nemes_to_1_hot_seq(string, nemes=\"phonemes\"):\n",
    "    string = '0' + string + '1'\n",
    "    l = phonemes if nemes == \"phonemes\" else graphemes\n",
    "    seq = []\n",
    "    for i in string:\n",
    "        vec = [0] * len(l)\n",
    "        vec[l.index(i)] = 1\n",
    "        seq.append(vec)\n",
    "\n",
    "    return torch.FloatTensor([seq])\n",
    "\n",
    "def one_hot_to_nemes(arr, nemes=\"phonemes\"):\n",
    "    seq = []\n",
    "    l = phonemes if nemes == \"phonemes\" else graphemes\n",
    "    for hot in arr:\n",
    "        x = torch.argmax(hot)\n",
    "        seq.append(l[x])\n",
    "    return seq\n",
    "\n",
    "class P2GDataset(Dataset):\n",
    "    def __init__(self, phoneme_file, device):\n",
    "        df = pd.read_csv(phoneme_file)\n",
    "        self.data = df.drop(df[df[\"phonemes\"].map(len) > 7].index)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p, g = self.data.iloc[idx]\n",
    "        return nemes_to_1_hot_seq(p, nemes = \"phonemes\").to(self.device), nemes_to_1_hot_seq(g, nemes = \"graphemes\").long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = 512\n",
    "# define model architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.GRU(len(phonemes), layer_size, 2, batch_first=True, bidirectional=True, dropout=0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * layer_size, layer_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # push vector through encoder\n",
    "        out, hidden = self.encoder(x)\n",
    "        # hidden is [4, 1, layer_size]\n",
    "        # this is because of bidirectionality * double stacked\n",
    "        # we want to grab the \"highest\" layers from the forwards and backwards directions\n",
    "        # dim 1 because hidden[3] and hidden[4] are both [1, layer_size] and we \n",
    "        # want a single batch that has 2*layer_size values\n",
    "        # print(hidden.shape)\n",
    "        hc = torch.cat((hidden[2], hidden[3]), dim=1)\n",
    "        hidden_for_init = self.fc(hc)\n",
    "\n",
    "        # return context vector\n",
    "        return out, hidden_for_init\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 2 from encoder state and 1 from decoder state (since not bidirectional)\n",
    "        self.energy = nn.Sequential(\n",
    "            nn.Linear(3*layer_size, layer_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Linear(layer_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, encoder_hiddens, decoder_hidden):\n",
    "        # encoder_hiddens is [1, L, layer_size*2] bc bidirectional\n",
    "        # decoder_hidden is [1, layer_size]\n",
    "        # 1 bc using batch first \n",
    "        num_encoder_hiddens = encoder_hiddens.shape[1]\n",
    "\n",
    "        # make it [1,1,layer_size]\n",
    "        decoder_hidden = torch.unsqueeze(decoder_hidden, 0)\n",
    "\n",
    "        # repeat along second dim to get [4, 1, layer_size]\n",
    "       \n",
    "        decoder_hiddens = decoder_hidden.squeeze(0).repeat(1, num_encoder_hiddens, 1)\n",
    "        \n",
    "        inputs = torch.cat((encoder_hiddens, decoder_hiddens), 2)\n",
    "\n",
    "        energy = self.energy(inputs)\n",
    "\n",
    "        attention = self.attention(energy)\n",
    "\n",
    "        # want a distribution of attention that sums to 1\n",
    "        return F.softmax(attention, dim=2)\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, attention):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.decoder = nn.GRU(len(graphemes) + 2*layer_size, layer_size, batch_first=True, bidirectional=False, dropout=0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(layer_size*3 + len(graphemes), len(graphemes))\n",
    "        )\n",
    "        \n",
    "    def forward(self, input, hidden_layer, encoder_hiddens):\n",
    "        \"\"\"\n",
    "        Since this function gets called once at a time rather than taking in\n",
    "        a sequence of vectors, we need to pass it the last output. This will be just\n",
    "        a vector of numbers that can be converted to the embedding representing that last output\n",
    "        \"\"\"\n",
    "        # [1,1,layer_size]\n",
    "        attention_vals = self.attention(encoder_hiddens, hidden_layer)\n",
    "        # encoder_hiddens [1,L,layer_size]\n",
    "        # this just multiplies each attention value against the appropriate vector\n",
    "        # and sums the weighted vectors\n",
    "        # will be [1, 1, layer_size]\n",
    "        \n",
    "        attention_vals = attention_vals.permute(0, 2, 1)\n",
    "        attended = torch.bmm(attention_vals, encoder_hiddens)\n",
    "#         torch.Size([1, 6, 1])\n",
    "# torch.Size([1, 6, 1024])\n",
    "        input = torch.cat((attended, input), dim=2)\n",
    "        out, hidden = self.decoder(input, hidden_layer)\n",
    "        # out[1] to get top hidden layer\n",
    "        input_for_fc = torch.cat((input, out), dim = 2)\n",
    "        # print(\"H\")\n",
    "        # (1x1564 and 1075x28)\n",
    "        return self.fc(input_for_fc), hidden\n",
    "\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.attention = Attention()\n",
    "        self.decoder = Decoder(self.attention)\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, in_seq, out_seq, tf_ratio=0.5):\n",
    "        out_len = out_seq.shape[1]\n",
    "        # storing the outputs of the sequence\n",
    "        outputs = torch.zeros(out_len, 1, len(graphemes)).to(self.device)\n",
    "\n",
    "        out_for_at, hidden = self.encoder(in_seq)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        out_seq = out_seq.squeeze(0)\n",
    "\n",
    "        input = out_seq[0].unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "        \n",
    "        for i in range(1, out_len):\n",
    "            out, hidden = self.decoder(input, hidden, out_for_at)\n",
    "            outputs[i] = out\n",
    "\n",
    "            if random.random() > tf_ratio:\n",
    "                # teacher forcing (make next input what the current output token should be)\n",
    "                input = out_seq[i].unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "            else:\n",
    "                x = input.argmax(1)[0]\n",
    "                input = torch.zeros(1, 1, len(graphemes)).to(self.device)\n",
    "                input[0][0][x] = 1\n",
    "                \n",
    "        return outputs\n",
    "\n",
    "    def pred_new(self, in_seq):\n",
    "        encoder_out_for_at, hidden = self.encoder(in_seq)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        input = torch.zeros(1, 1, len(graphemes)).to(self.device)\n",
    "        outs = []\n",
    "        while True:\n",
    "            out, hidden = self.decoder(input, hidden, encoder_out_for_at)\n",
    "            outs.append(out)\n",
    "            if len(outs) > 30:\n",
    "                break\n",
    "            x = input.argmax(1)[0]\n",
    "            input = torch.zeros(1, 1, len(graphemes)).to(self.device)\n",
    "            input[0][0][x] = 1\n",
    "            if one_hot_to_nemes(out) == ['1']:\n",
    "                break\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq(\n",
      "  (encoder): Encoder(\n",
      "    (encoder): GRU(51, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (attention): Attention(\n",
      "    (energy): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=512, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (attention): Linear(in_features=512, out_features=1, bias=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (energy): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=512, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "      (attention): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (decoder): GRU(1052, 512, batch_first=True, dropout=0.5)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=1564, out_features=28, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\psi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"training\"\"\"\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 100\n",
    "model = seq2seq(device).to(device)\n",
    "# what a beautiful architecture\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "dataset = P2GDataset(\"phonemes-words.csv\", device)\n",
    "train, test = random_split(dataset, [35000, len(dataset)-35000])\n",
    "dataloader = DataLoader(dataset=train, batch_size=1)\n",
    "print(len(test))\n",
    "\n",
    "def get_0_1_accuracy(test_set, model):\n",
    "    correct = 0\n",
    "    dataloader = DataLoader(dataset=test_set, batch_size=1)\n",
    "    for (in_seq, out_seq) in dataloader:\n",
    "        # print(out_seq.shape)\n",
    "        # break\n",
    "        prediction = model.pred_new(in_seq[0])\n",
    "        # print(in_seq[0].shape)\n",
    "        true = \"\".join(one_hot_to_nemes(out_seq[0][0], \"graphemes\"))[1:-1]\n",
    "        print(true)\n",
    "        # print(one_hot_to_nemes(prediction, \"graphemes\"))\n",
    "        # print(prediction)\n",
    "        pred = \"\".join(one_hot_to_nemes(prediction, \"graphemes\"))[0:-1]\n",
    "        print(pred)\n",
    "        \n",
    "        if true == pred:\n",
    "            correct+= 1\n",
    "    if correct == 0:\n",
    "        return correct\n",
    "    return correct/len(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10221868\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7f82de313fd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtot_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_losses = []\n",
    "writer = SummaryWriter(\"tensorboard_data\")\n",
    "\n",
    "_, mini_test = random_split(test, [20, len(test)-20])\n",
    "# 15 quite good\n",
    "for epoch in range(30):\n",
    "    tot_loss = 0\n",
    "    for (in_seq, out_seq) in dataloader:\n",
    "        in_seq = in_seq.squeeze(0)\n",
    "        out_seq = out_seq.squeeze(0)\n",
    "        model_output = model(in_seq, out_seq)\n",
    "        model_output = model_output[1:]\n",
    "        model_output = model_output.squeeze(1)\n",
    "        out_seq = out_seq.squeeze(0)[1:]\n",
    "        loss = loss_func(model_output, out_seq.argmax(1).to(device))\n",
    "        tot_loss+=loss.detach().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    tot_loss/=len(dataset)\n",
    "    writer.add_scalar(\"tensorboard_data/acc\", get_0_1_accuracy(mini_test, model), epoch)\n",
    "    writer.add_scalar(\"tensorboard_data\", loss.detach().item(), epoch)\n",
    "    avg_losses.append(tot_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = P2GDataset(\"data.csv\", \"cuda\")\n",
    "p, g = dataset[0]\n",
    "\n",
    "print(one_hot_to_nemes(p[0], \"phonemes\"))\n",
    "p.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celled\n",
      "ssssssssssssssssssssssssssssss\n",
      "pubes\n",
      "ssssssssssssssssssssssssssssss\n",
      "janice\n",
      "ssssssssssssssssssssssssssssss\n",
      "bonaci\n",
      "ssssssssssssssssssssssssssssss\n",
      "truckle\n",
      "ssssssssssssssssssssssssssssss\n",
      "rangoon\n",
      "ssssssssssssssssssssssssssssss\n",
      "caddis\n",
      "ssssssssssssssssssssssssssssss\n",
      "mant\n",
      "ysssssssssssssssssssssssssssss\n",
      "rookery\n",
      "kkkkkkkkkkkkkkkkkkkkkkkkkkkkkk\n",
      "awhile\n",
      "ssssssssssssssssssssssssssssss\n",
      "citrus\n",
      "sssaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "haaf\n",
      "ssssssssssssssssssssssssssssss\n",
      "quintain\n",
      "ssssssssssssssssssssssssssssss\n",
      "etooth\n",
      "ssssssssssssssssssssssssssssss\n",
      "aum\n",
      "ssssssssssssssssssssssssssssss\n",
      "meshy\n",
      "ssssssssssssssssssssssssssssss\n",
      "jocund\n",
      "ssssssssssssssssssssssssssssss\n",
      "dyarchy\n",
      "ssssssssssssssssssssssssssssss\n",
      "banding\n",
      "ssssssssssssssssssssssssssssss\n",
      "short\n",
      "ssssssssssssssssssssssssssssss\n",
      "bari\n",
      "ssssssssssssssssssssssssssssss\n",
      "tooling\n",
      "ssssssssssssssssssssssssssssss\n",
      "gal\n",
      "ssssssssssssssssssssssssssssss\n",
      "budding\n",
      "ssssssssssssssssssssssssssssss\n",
      "perigee\n",
      "ssssssssssssssssssssssssssssss\n",
      "laith\n",
      "ssssssssssssssssssssssssssssss\n",
      "wallsend\n",
      "ssssssssssssssssssssssssssssss\n",
      "infuse\n",
      "ssssssssssssssssssssssssssssss\n",
      "bellpull\n",
      "ssssssssssssssssssssssssssssss\n",
      "tittup\n",
      "ysssssssssssssssssssssssssssss\n",
      "delicate\n",
      "ssssssssssssssssssssssssssssss\n",
      "bogwood\n",
      "ssssssssssssssssssssssssssssss\n",
      "elias\n",
      "ssssssssssssssssssssssssssssss\n",
      "kief\n",
      "kkssssssssssssssssssssssssssss\n",
      "thieving\n",
      "ssssssssssssssssssssssssssssss\n",
      "crandall\n",
      "ssssssssssssssssssssssssssssss\n",
      "cheapen\n",
      "ssssssssssssssssssssssssssssss\n",
      "dogcart\n",
      "ssssssssssssssssssssssssssssss\n",
      "lions\n",
      "ssssssssssssssssssssssssssssss\n",
      "equid\n",
      "ssssssssssssssssssssssssssssss\n",
      "surge\n",
      "ssssssssssssssssssssssssssssss\n",
      "wanker\n",
      "ssssssssssssssssssssssssssssss\n",
      "nocturn\n",
      "ssssssssssssssssssssssssssssss\n",
      "dwayne\n",
      "ssssssssssssssssssssssssssssss\n",
      "heritor\n",
      "ssssssssssssssssssssssssssssss\n",
      "rouche\n",
      "ssssssssssssssssssssssssssssss\n",
      "buddy\n",
      "ssssssssssssssssssssssssssssss\n",
      "ambon\n",
      "ssssssssssssssssssssssssssssss\n",
      "shadoof\n",
      "ssssssssssssssssssssssssssssss\n",
      "chockfull\n",
      "ssssssssssssssssssssssssssssss\n",
      "health\n",
      "ssssssssssssssssssssssssssssss\n",
      "brand\n",
      "ssssssssssssssssssssssssssssss\n",
      "shipman\n",
      "ssssssssssssssssssssssssssssss\n",
      "madly\n",
      "ssssssssssssssssssssssssssssss\n",
      "elliott\n",
      "ssssssssssssssssssssssssssssss\n",
      "snaffle\n",
      "ssssssssssssssssssssssssssssss\n",
      "buqsha\n",
      "ssssssssssssssssssssssssssssss\n",
      "gosport\n",
      "ssssssssssssssssssssssssssssss\n",
      "traprock\n",
      "ssssssssssssssssssssssssssssss\n",
      "sawbill\n",
      "ssssssssssssssssssssssssssssss\n",
      "anaheim\n",
      "ssssssssssssssssssssssssssssss\n",
      "induct\n",
      "ssssssssssssssssssssssssssssss\n",
      "octagon\n",
      "ssssssssssssssssssssssssssssss\n",
      "guttering\n",
      "ysssssssssssssssssssssssssssss\n",
      "sparling\n",
      "ssssssssssssssssssssssssssssss\n",
      "size\n",
      "ssssssssssssssssssssssssssssss\n",
      "tucson\n",
      "ssssssssssssssssssssssssssssss\n",
      "stamnos\n",
      "ssssssssssssssssssssssssssssss\n",
      "fuck\n",
      "skssssssssssssssssssssssssssss\n",
      "efik\n",
      "ssssssssssssssssssssssssssssss\n",
      "worksheet\n",
      "ssssssssssssssssssssssssssssss\n",
      "cellarer\n",
      "ysssssssssssssssssssssssssssss\n",
      "interne\n",
      "ssssaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "lisa\n",
      "ssssssssssssssssssssssssssssss\n",
      "allen\n",
      "ssssssssssssssssssssssssssssss\n",
      "rudish\n",
      "ssssssssssssssssssssssssssssss\n",
      "impi\n",
      "ssssssssssssssssssssssssssssss\n",
      "interim\n",
      "yyaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "airscrew\n",
      "ssssssssssssssssssssssssssssss\n",
      "swamped\n",
      "ssssssssssssssssssssssssssssss\n",
      "mahound\n",
      "ssssssssssssssssssssssssssssss\n",
      "ahimsa\n",
      "ssssssssssssssssssssssssssssss\n",
      "tommy\n",
      "ssssssssssssssssssssssssssssss\n",
      "totem\n",
      "ssssssssssssssssssssssssssssss\n",
      "dahoon\n",
      "ssssssssssssssssssssssssssssss\n",
      "drung\n",
      "ssssssssssssssssssssssssssssss\n",
      "rodent\n",
      "ssssssssssssssssssssssssssssss\n",
      "squidgy\n",
      "ssssssssssssssssssssssssssssss\n",
      "apply\n",
      "ssssssssssssssssssssssssssssss\n",
      "guelph\n",
      "ssssssssssssssssssssssssssssss\n",
      "remnant\n",
      "ssssssssssssssssssssssssssssss\n",
      "teether\n",
      "ssssssssssssssssssssssssssssss\n",
      "mulciber\n",
      "ssssssssssssssssssssssssssssss\n",
      "vinland\n",
      "ssssssssssssssssssssssssssssss\n",
      "organum\n",
      "yossssssssssssssssssssssssssss\n",
      "grantor\n",
      "sssaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "haply\n",
      "ssssssssssssssssssssssssssssss\n",
      "smaragd\n",
      "ssssssssssssssssssssssssssssss\n",
      "antiquer\n",
      "ssssssssssssssssssssssssssssss\n",
      "obsequy\n",
      "ssssssssssssssssssssssssssssss\n",
      "faux\n",
      "ssssssssssssssssssssssssssssss\n",
      "bamboo\n",
      "ssssssssssssssssssssssssssssss\n",
      "ashore\n",
      "ssssssssssssssssssssssssssssss\n",
      "boater\n",
      "ysssssssssssssssssssssssssssss\n",
      "detailed\n",
      "ssssssssssssssssssssssssssssss\n",
      "drawee\n",
      "ssssssssssssssssssssssssssssss\n",
      "buckish\n",
      "ssssssssssssssssssssssssssssss\n",
      "algid\n",
      "ssssssssssssssssssssssssssssss\n",
      "dishevel\n",
      "ssssssssssssssssssssssssssssss\n",
      "stockton\n",
      "ssssssssssssssssssssssssssssss\n",
      "prink\n",
      "ssssssssssssssssssssssssssssss\n",
      "clive\n",
      "ssssssssssssssssssssssssssssss\n",
      "mutchkin\n",
      "ssssssssssssssssssssssssssssss\n",
      "amniote\n",
      "ssssssssssssssssssssssssssssss\n",
      "avalon\n",
      "ssssssssssssssssssssssssssssss\n",
      "cirrus\n",
      "saaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "adams\n",
      "ssssssssssssssssssssssssssssss\n",
      "sarcode\n",
      "ssssssssssssssssssssssssssssss\n",
      "drum\n",
      "ssssssssssssssssssssssssssssss\n",
      "cabal\n",
      "ssssssssssssssssssssssssssssss\n",
      "vigour\n",
      "ssssssssssssssssssssssssssssss\n",
      "da\n",
      "ysssssssssssssssssssssssssssss\n",
      "chignon\n",
      "ssssssssssssssssssssssssssssss\n",
      "chinone\n",
      "ssssssssssssssssssssssssssssss\n",
      "lenore\n",
      "ssssssssssssssssssssssssssssss\n",
      "teaberry\n",
      "ssssssssssssssssssssssssssssss\n",
      "alidade\n",
      "ssssssssssssssssssssssssssssss\n",
      "corpora\n",
      "yyyaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "enolize\n",
      "ssssssssssssssssssssssssssssss\n",
      "travel\n",
      "ssssssssssssssssssssssssssssss\n",
      "ge\n",
      "ssssssssssssssssssssssssssssss\n",
      "pairing\n",
      "ssssssssssssssssssssssssssssss\n",
      "balkan\n",
      "ssssssssssssssssssssssssssssss\n",
      "moses\n",
      "ssssssssssssssssssssssssssssss\n",
      "tafia\n",
      "ssssssssssssssssssssssssssssss\n",
      "pock\n",
      "ssssssssssssssssssssssssssssss\n",
      "toast\n",
      "ssssssssssssssssssssssssssssss\n",
      "buskined\n",
      "ssssssssssssssssssssssssssssss\n",
      "amylase\n",
      "ssssssssssssssssssssssssssssss\n",
      "harewood\n",
      "ssssssssssssssssssssssssssssss\n",
      "seagull\n",
      "ssssssssssssssssssssssssssssss\n",
      "cornell\n",
      "ssssssssssssssssssssssssssssss\n",
      "seleucid\n",
      "ssssssssssssssssssssssssssssss\n",
      "shadowy\n",
      "ssssssssssssssssssssssssssssss\n",
      "midships\n",
      "ssssssssssssssssssssssssssssss\n",
      "meshed\n",
      "ssssssssssssssssssssssssssssss\n",
      "prao\n",
      "ssssssssssssssssssssssssssssss\n",
      "perp\n",
      "saaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "estrous\n",
      "ssssssssssssssssssssssssssssss\n",
      "yonkers\n",
      "osssssssssssssssssssssssssssss\n",
      "spathe\n",
      "ssssssssssssssssssssssssssssss\n",
      "splutter\n",
      "ssssssssssssssssssssssssssssss\n",
      "incurve\n",
      "ssssssssssssssssssssssssssssss\n",
      "oarlock\n",
      "ssssssssssssssssssssssssssssss\n",
      "dismal\n",
      "ssssssssssssssssssssssssssssss\n",
      "poach\n",
      "ssssssssssssssssssssssssssssss\n",
      "phage\n",
      "ssssssssssssssssssssssssssssss\n",
      "marvy\n",
      "ssssssssssssssssssssssssssssss\n",
      "chremzel\n",
      "ssssssssssssssssssssssssssssss\n",
      "iconic\n",
      "ssssssssssssssssssssssssssssss\n",
      "oleic\n",
      "ssssssssssssssssssssssssssssss\n",
      "burden\n",
      "ssssssssssssssssssssssssssssss\n",
      "windway\n",
      "ssssssssssssssssssssssssssssss\n",
      "effable\n",
      "ysssssssssssssssssssssssssssss\n",
      "receive\n",
      "ssssssssssssssssssssssssssssss\n",
      "ginkgo\n",
      "ssssssssssssssssssssssssssssss\n",
      "aloofness\n",
      "ssssssssssssssssssssssssssssss\n",
      "enoch\n",
      "skssssssssssssssssssssssssssss\n",
      "rust\n",
      "ssssssssssssssssssssssssssssss\n",
      "nary\n",
      "ssssssssssssssssssssssssssssss\n",
      "jolly\n",
      "ssssssssssssssssssssssssssssss\n",
      "chlorous\n",
      "ssssssssssssssssssssssssssssss\n",
      "tradeoff\n",
      "ssssssssssssssssssssssssssssss\n",
      "acetal\n",
      "ssssssssssssssssssssssssssssss\n",
      "hel\n",
      "ssssssssssssssssssssssssssssss\n",
      "palmar\n",
      "ysssssssssssssssssssssssssssss\n",
      "praecipe\n",
      "sssaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "pang\n",
      "ssssssssssssssssssssssssssssss\n",
      "stere\n",
      "ysssssssssssssssssssssssssssss\n",
      "redcoat\n",
      "ssssssssssssssssssssssssssssss\n",
      "clop\n",
      "ssssssssssssssssssssssssssssss\n",
      "whoso\n",
      "ssssssssssssssssssssssssssssss\n",
      "drisheen\n",
      "ssssssssssssssssssssssssssssss\n",
      "thither\n",
      "ssssssssssssssssssssssssssssss\n",
      "sialid\n",
      "ssssssssssssssssssssssssssssss\n",
      "roughhew\n",
      "ssssssssssssssssssssssssssssss\n",
      "milker\n",
      "ysssssssssssssssssssssssssssss\n",
      "fatwood\n",
      "ssssssssssssssssssssssssssssss\n",
      "chenier\n",
      "ssssssssssssssssssssssssssssss\n",
      "audit\n",
      "ssssssssssssssssssssssssssssss\n",
      "espial\n",
      "ssssssssssssssssssssssssssssss\n",
      "shant\n",
      "ssssssssssssssssssssssssssssss\n",
      "cinnamene\n",
      "ssssssssssssssssssssssssssssss\n",
      "irrorate\n",
      "ysssssssssssssssssssssssssssss\n",
      "vibe\n",
      "ssssssssssssssssssssssssssssss\n",
      "meu\n",
      "ssssssssssssssssssssssssssssss\n",
      "sachem\n",
      "ssssssssssssssssssssssssssssss\n",
      "upper\n",
      "ykaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "rumdum\n",
      "ssssssssssssssssssssssssssssss\n",
      "turd\n",
      "ssssssssssssssssssssssssssssss\n",
      "stupid\n",
      "ssssssssssssssssssssssssssssss\n",
      "numeral\n",
      "ysssssssssssssssssssssssssssss\n",
      "abuse\n",
      "ssssssssssssssssssssssssssssss\n",
      "japan\n",
      "ssssssssssssssssssssssssssssss\n",
      "sunroom\n",
      "ssssssssssssssssssssssssssssss\n",
      "hatbox\n",
      "ssssssssssssssssssssssssssssss\n",
      "outman\n",
      "ssssssssssssssssssssssssssssss\n",
      "uzbek\n",
      "ssssssssssssssssssssssssssssss\n",
      "handoff\n",
      "ssssssssssssssssssssssssssssss\n",
      "gae\n",
      "ssssssssssssssssssssssssssssss\n",
      "despond\n",
      "ssssssssssssssssssssssssssssss\n",
      "seasoning\n",
      "ssssssssssssssssssssssssssssss\n",
      "malaise\n",
      "ssssssssssssssssssssssssssssss\n",
      "huppah\n",
      "ysssssssssssssssssssssssssssss\n",
      "skedaddle\n",
      "ssssssssssssssssssssssssssssss\n",
      "honda\n",
      "ssssssssssssssssssssssssssssss\n",
      "attuned\n",
      "ssssssssssssssssssssssssssssss\n",
      "brassard\n",
      "ssssssssssssssssssssssssssssss\n",
      "actress\n",
      "ssssssssssssssssssssssssssssss\n",
      "eon\n",
      "ssssssssssssssssssssssssssssss\n",
      "waco\n",
      "ssssssssssssssssssssssssssssss\n",
      "geek\n",
      "kkssssssssssssssssssssssssssss\n",
      "webbing\n",
      "ssssssssssssssssssssssssssssss\n",
      "knave\n",
      "ssssssssssssssssssssssssssssss\n",
      "rank\n",
      "ssssssssssssssssssssssssssssss\n",
      "culex\n",
      "ssssssssssssssssssssssssssssss\n",
      "yell\n",
      "ssssssssssssssssssssssssssssss\n",
      "cassette\n",
      "ssssssssssssssssssssssssssssss\n",
      "headlamp\n",
      "ssssssssssssssssssssssssssssss\n",
      "assay\n",
      "ssssssssssssssssssssssssssssss\n",
      "wrapt\n",
      "ssssssssssssssssssssssssssssss\n",
      "rhenic\n",
      "ssssssssssssssssssssssssssssss\n",
      "policy\n",
      "ssssssssssssssssssssssssssssss\n",
      "hank\n",
      "ssssssssssssssssssssssssssssss\n",
      "scrappy\n",
      "ssssssssssssssssssssssssssssss\n",
      "corrie\n",
      "ksssssssssssssssssssssssssssss\n",
      "juicehead\n",
      "ssssssssssssssssssssssssssssss\n",
      "quibble\n",
      "ysssssssssssssssssssssssssssss\n",
      "quayside\n",
      "ssssssssssssssssssssssssssssss\n",
      "affricate\n",
      "ssssssssssssssssssssssssssssss\n",
      "snuck\n",
      "ssssssssssssssssssssssssssssss\n",
      "tamil\n",
      "ssssssssssssssssssssssssssssss\n",
      "habiru\n",
      "ssssssssssssssssssssssssssssss\n",
      "ricochet\n",
      "ssssssssssssssssssssssssssssss\n",
      "kabob\n",
      "ysssssssssssssssssssssssssssss\n",
      "lisbon\n",
      "sssaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "inky\n",
      "ssssssssssssssssssssssssssssss\n",
      "certainty\n",
      "ssssssssssssssssssssssssssssss\n",
      "vomica\n",
      "ssssssssssssssssssssssssssssss\n",
      "borasca\n",
      "osssssssssssssssssssssssssssss\n",
      "knapsack\n",
      "ssssssssssssssssssssssssssssss\n",
      "gent\n",
      "ssssssssssssssssssssssssssssss\n",
      "pienaar\n",
      "ssssssssssssssssssssssssssssss\n",
      "mumble\n",
      "ysssssssssssssssssssssssssssss\n",
      "ossa\n",
      "ssssssssssssssssssssssssssssss\n",
      "zingy\n",
      "ssssssssssssssssssssssssssssss\n",
      "laudable\n",
      "ysssssssssssssssssssssssssssss\n",
      "feeler\n",
      "ssssssssssssssssssssssssssssss\n",
      "crowd\n",
      "ssssssssssssssssssssssssssssss\n",
      "rhachis\n",
      "ssssssssssssssssssssssssssssss\n",
      "yaba\n",
      "ssssssssssssssssssssssssssssss\n",
      "nubia\n",
      "ssssssssssssssssssssssssssssss\n",
      "voiture\n",
      "ysssssssssssssssssssssssssssss\n",
      "promote\n",
      "ssssssssssssssssssssssssssssss\n",
      "weighty\n",
      "ssssssssssssssssssssssssssssss\n",
      "galgal\n",
      "ssssssssssssssssssssssssssssss\n",
      "tunica\n",
      "ssssssssssssssssssssssssssssss\n",
      "hotpress\n",
      "ssssssssssssssssssssssssssssss\n",
      "lotic\n",
      "ssssssssssssssssssssssssssssss\n",
      "scoffer\n",
      "ssssssssssssssssssssssssssssss\n",
      "quechuan\n",
      "ssssssssssssssssssssssssssssss\n",
      "peele\n",
      "ssssssssssssssssssssssssssssss\n",
      "walrus\n",
      "ysssssssssssssssssssssssssssss\n",
      "aedoeagus\n",
      "ssssssssssssssssssssssssssssss\n",
      "petty\n",
      "ssssssssssssssssssssssssssssss\n",
      "putto\n",
      "ssssssssssssssssssssssssssssss\n",
      "typhon\n",
      "ssssssssssssssssssssssssssssss\n",
      "umist\n",
      "ssssssssssssssssssssssssssssss\n",
      "drooly\n",
      "ssssssssssssssssssssssssssssss\n",
      "sanious\n",
      "ssssssssssssssssssssssssssssss\n",
      "camphire\n",
      "ssssssssssssssssssssssssssssss\n",
      "bowtel\n",
      "ssssssssssssssssssssssssssssss\n",
      "zincate\n",
      "ssssssssssssssssssssssssssssss\n",
      "malarkey\n",
      "ssssssssssssssssssssssssssssss\n",
      "suntanned\n",
      "ssssssssssssssssssssssssssssss\n",
      "troop\n",
      "ssssssssssssssssssssssssssssss\n",
      "affrayer\n",
      "yyaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "course\n",
      "ssssssssssssssssssssssssssssss\n",
      "beginning\n",
      "ssssssssssssssssssssssssssssss\n",
      "sorely\n",
      "ssssssssssssssssssssssssssssss\n",
      "ac\n",
      "ssssssssssssssssssssssssssssss\n",
      "erratic\n",
      "ssssssssssssssssssssssssssssss\n",
      "mf\n",
      "ssssssssssssssssssssssssssssss\n",
      "holy\n",
      "ssssssssssssssssssssssssssssss\n",
      "bindi\n",
      "ssssssssssssssssssssssssssssss\n",
      "giddiness\n",
      "ssssssssssssssssssssssssssssss\n",
      "walloping\n",
      "ssssssssssssssssssssssssssssss\n",
      "humbug\n",
      "ssssssssssssssssssssssssssssss\n",
      "adsorb\n",
      "ssssssssssssssssssssssssssssss\n",
      "terrible\n",
      "ysssssssssssssssssssssssssssss\n",
      "sidearm\n",
      "ssssssssssssssssssssssssssssss\n",
      "dumpling\n",
      "ssssssssssssssssssssssssssssss\n",
      "spark\n",
      "ssssssssssssssssssssssssssssss\n",
      "ayous\n",
      "ssssssssssssssssssssssssssssss\n",
      "karate\n",
      "ssssssssssssssssssssssssssssss\n",
      "caddy\n",
      "ssssssssssssssssssssssssssssss\n",
      "baric\n",
      "ssssssssssssssssssssssssssssss\n",
      "dodge\n",
      "ssssssssssssssssssssssssssssss\n",
      "podgy\n",
      "ssssssssssssssssssssssssssssss\n",
      "focus\n",
      "ssssssssssssssssssssssssssssss\n",
      "cookware\n",
      "ysssssssssssssssssssssssssssss\n",
      "amorous\n",
      "yossssssssssssssssssssssssssss\n",
      "blotto\n",
      "ssssssssssssssssssssssssssssss\n",
      "just\n",
      "ssssssssssssssssssssssssssssss\n",
      "waterloo\n",
      "ysssssssssssssssssssssssssssss\n",
      "charged\n",
      "ssssssssssssssssssssssssssssss\n",
      "stoppage\n",
      "ssssssssssssssssssssssssssssss\n",
      "oughtlins\n",
      "ssssssssssssssssssssssssssssss\n",
      "exurb\n",
      "ssssssssssssssssssssssssssssss\n",
      "sucre\n",
      "ssssssssssssssssssssssssssssss\n",
      "boric\n",
      "ssssssssssssssssssssssssssssss\n",
      "thorite\n",
      "ssssssssssssssssssssssssssssss\n",
      "flaunt\n",
      "ssssssssssssssssssssssssssssss\n",
      "billbug\n",
      "ssssssssssssssssssssssssssssss\n",
      "festuca\n",
      "ssssssssssssssssssssssssssssss\n",
      "succory\n",
      "kkssssssssssssssssssssssssssss\n",
      "parthian\n",
      "ssssssssssssssssssssssssssssss\n",
      "dying\n",
      "ssssssssssssssssssssssssssssss\n",
      "rowdyish\n",
      "ssssssssssssssssssssssssssssss\n",
      "alicia\n",
      "ysssssssssssssssssssssssssssss\n",
      "homy\n",
      "ssssssssssssssssssssssssssssss\n",
      "venison\n",
      "ssssssssssssssssssssssssssssss\n",
      "bousy\n",
      "ssssssssssssssssssssssssssssss\n",
      "vagrom\n",
      "osssssssssssssssssssssssssssss\n",
      "alan\n",
      "ssssssssssssssssssssssssssssss\n",
      "scaup\n",
      "ssssssssssssssssssssssssssssss\n",
      "mason\n",
      "ssssssssssssssssssssssssssssss\n",
      "turps\n",
      "ssssssssssssssssssssssssssssss\n",
      "iso\n",
      "ssssssssssssssssssssssssssssss\n",
      "hunger\n",
      "osssssssssssssssssssssssssssss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-eaec6fe630c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# print_preds(\"data.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_0_1_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# 36 great for train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# print(test[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-20951e453ff0>\u001b[0m in \u001b[0;36mget_0_1_accuracy\u001b[1;34m(test_set, model)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# print(out_seq.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;31m# print(in_seq[0].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mtrue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_hot_to_nemes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"graphemes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-1d9509ba9121>\u001b[0m in \u001b[0;36mpred_new\u001b[1;34m(self, in_seq)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpred_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mencoder_out_for_at\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraphemes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-1d9509ba9121>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# push vector through encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;31m# hidden is [4, 1, layer_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# this is because of bidirectionality * double stacked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    835\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 837\u001b[1;33m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    838\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    839\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "def print_preds(path):\n",
    "    global p\n",
    "    print(one_hot_to_nemes(p[0], \"phonemes\"))\n",
    "    s = model.pred_new(p)\n",
    "    \n",
    "    print(one_hot_to_nemes(s, \"graphemes\"))\n",
    "\n",
    "# print_preds(\"data.csv\")\n",
    "print(get_0_1_accuracy(test, model))\n",
    "# 36 great for train set\n",
    "# print(test[0])\n",
    "# print(one_hot_to_graphemes(torch.FloatTensor([[3,2,1],[0,0,1],[0,0,1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7538,  0.1556,  0.0660,  ..., -0.0320, -0.0349, -0.0978],\n",
       "        [-0.0865,  0.0106, -0.2135,  ...,  0.0184,  0.0110,  0.0298],\n",
       "        [ 0.1121, -0.0444, -0.1053,  ...,  0.0754, -0.0277,  0.0016],\n",
       "        ...,\n",
       "        [-0.1423,  0.2361, -0.6792,  ...,  0.0250, -0.0080, -0.0016],\n",
       "        [ 0.0889,  0.2124,  0.0398,  ..., -0.0352,  0.0361, -0.0685],\n",
       "        [-0.2383,  0.0234, -0.0281,  ...,  0.0096, -0.0413, -0.0148]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.encoder.weight_ih_l0"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99cbaf4c95006fdb5479aa5c5d34049fe62ad37116b43f87b95900136f6751f7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
